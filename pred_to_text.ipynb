{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dee3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404d3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # 전체 텍스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f052624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('predict_NER_electra_final_ids_rev_21.11.06-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4246ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3817aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ner_to_index_rev_1030.json\", 'rb') as f:\n",
    "        ner_to_index = json.load(f)\n",
    "index_to_ner = {}\n",
    "for key, value in ner_to_index.items():\n",
    "    index_to_ner[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc37d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_input : 인코딩된 input 데이터, pred : 예측 데이터\n",
    "# return 값은 B, I 라벨링된 텍스트 데이터\n",
    "def pred_to_BI_text(enc_input, pred):\n",
    "    idx = 0\n",
    "    label_texts = ''\n",
    "    for i in range(len(pred)):\n",
    "        if i >= len(enc_input):\n",
    "            label_texts += tokenizer.decode(enc_input[prev_idx:idx]).replace('#', '')\n",
    "            break\n",
    "        num = int(pred[i])\n",
    "        if (num) > 4:\n",
    "            label_texts += tokenizer.decode(enc_input[idx:i]).replace('#', '') + ' '\n",
    "            label_text = \"<{}:{}>\".format(tokenizer.decode(enc_input[i]).replace(' ', '').replace('#', ''), index_to_ner[num])\n",
    "            label_texts += label_text + ' '\n",
    "            idx = i+1\n",
    "            \n",
    "    label_texts = ' '.join(label_texts.split())\n",
    "    return label_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08a98cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_input : 인코딩된 input 데이터, pred : 예측 데이터\n",
    "# return 값은 코드 라벨링된 텍스트 데이터\n",
    "def pred_to_text(enc_input, pred): \n",
    "    idx = 0\n",
    "    prev_idx = 0\n",
    "    label_texts = ''\n",
    "    B_chk = False\n",
    "    while idx < len(pred):\n",
    "        if idx >= len(enc_input):\n",
    "            label_texts += tokenizer.decode(enc_input[prev_idx:idx]).replace('#', '')\n",
    "            break\n",
    "        num = int(pred[idx])\n",
    "        if num > 4: # 태그인 경우\n",
    "            label_texts += tokenizer.decode(enc_input[prev_idx:idx]).replace('#', '')  # 태그가 등장하기 전 인덱스 값들을 텍스트로 변환\n",
    "            text = ''\n",
    "            while int(pred[idx]) > 4: # 다음 인덱스들이 태그가 아닐 때까지 반복\n",
    "                if idx >= len(enc_input):\n",
    "                    break\n",
    "                if not B_chk: # B_chk 가 False 이면 (B 태그가 등장하지 않았을 때)\n",
    "                    if 'B▁' in index_to_ner[int(pred[idx])]: # B태그라면\n",
    "                        code = index_to_ner[int(pred[idx])][2:] # 코드를 저장\n",
    "                        text += tokenizer.decode(int(enc_input[idx])).replace('#', '').replace(' ', '') # 인덱스 값을 텍스트로 변환\n",
    "                        B_chk = True # 이전 인덱스 값이 B태그라는 표시\n",
    "                        idx += 1 # 다음 인덱스 확인\n",
    "                    elif 'I▁' in index_to_ner[int(pred[idx])]: # I태그라면\n",
    "                        idx += 1 # 다음 인덱스 확인\n",
    "                        B_chk = False # 이전 인덱스 값이 B태그가 아니라는 표시\n",
    "                        break\n",
    "                    else: # B, I 태그가 아니라면\n",
    "                        text += tokenizer.decode(int(enc_input[idx])).replace('#', '').replace(' ', '') # 인덱스 값을 텍스트로 변환\n",
    "                        B_chk = False # 이전 인덱스 값이 B태그가 아니라는 표시\n",
    "                        break\n",
    "                elif B_chk: # B_chk 가 True 이면 (이전에 B 태그가 등장했을 때)\n",
    "                    if 'I▁' in index_to_ner[int(pred[idx])]: # I 태그라면\n",
    "                        text += tokenizer.decode(int(enc_input[idx])).replace('#', '').replace(' ', '') # 인덱스 값을 텍스트로 변환\n",
    "                        idx += 1 # 다음 인덱스 확인\n",
    "                        B_chk = False # 이전 인덱스 값이 B태그가 아니라는 표시\n",
    "                    elif 'B▁' in index_to_ner[int(pred[idx])]:\n",
    "                        B_chk = False # 이전 인덱스 값이 B태그가 아니라는 표시\n",
    "                        break\n",
    "                    else: # B, I 태그가 아니라면\n",
    "                        B_chk = False # # 이전 인덱스 값이 B태그가 아니라는 표시\n",
    "                        break\n",
    "            if len(text) > 0:\n",
    "                try:\n",
    "                    label_text = \"<{}:{}>\".format(text, code)\n",
    "                except:\n",
    "                    print(pred)\n",
    "                    print(index_to_ner[int(pred[idx])])\n",
    "                    break\n",
    "                label_texts += ' ' + label_text + ' '\n",
    "            prev_idx = idx\n",
    "        else: # 태그가 아닌 경우\n",
    "            idx += 1\n",
    "    \n",
    "    label_texts = ' '.join(label_texts.split())\n",
    "    return label_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3248d778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37088/37088 [02:42<00:00, 228.78it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_label_texts = []\n",
    "for i in tqdm(range(len(df_pred))):\n",
    "    enc_input = tokenizer.encode(df_pred['input'].iloc[i])\n",
    "    pred = df_pred['predict'].iloc[i][1:-1].replace(',', '').split()\n",
    "    pred_label_texts.append(pred_to_text(enc_input, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7ba7d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37088/37088 [02:44<00:00, 224.87it/s]\n"
     ]
    }
   ],
   "source": [
    "target_label_texts = []\n",
    "for i in tqdm(range(len(df_pred))):\n",
    "    enc_input = tokenizer.encode(df_pred['input'].iloc[i])\n",
    "    pred = df_pred['target'].iloc[i][1:-1].replace(',', '').split()\n",
    "    target_label_texts.append(pred_to_text(enc_input, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75705ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_texts = []\n",
    "# for i in tqdm(range(len(df_pred))):\n",
    "#     enc_input = tokenizer.encode(df_pred['input'].iloc[i])[1:-1]\n",
    "#     pred = df_pred['predict'].iloc[i][1:-1].replace(',', '').split()\n",
    "#     label_texts.append(pred_to_BI_text(enc_input, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f1c23ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 국립소록도병원이 <EMR:EE01> system을 구축한 2013년 10월 1일부터 2015년 12월 31 일까지의 <구강:LC14> 보건실을 내원한 6331명의 <한센:LC02> 력자들의 <진료:ND05> 분석을 통해 성 별 [UNK] 연령별50대이하 [UNK] 5160 [UNK] 6170 [UNK] 7181 [UNK] 80대이상 [UNK] 병동과 마을 <환자:SG02> 분류를 바탕으로 Clark의 <구강:LC14> 병 <예방:SE03> 분류에 의하여 [UNK] 1차 <예방:SE03> 진료와 <2차:NB04> <예방:SE03> 진료 및 <3차:EI03> <예방:SE03> 진료로 나누어 분류하여 <구강:LC14> 진료통계를 조사할 계획이다 [SEP]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input = tokenizer.encode(df_pred['input'].iloc[300])\n",
    "pred = df_pred['predict'].iloc[300][1:-1].replace(',', '').split()\n",
    "(pred_to_text(enc_input, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fe013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 245,12344\n",
    "df_pred['input'].iloc[7777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec1e9395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] <유연:NC09> <촉각센서신호처리:OB01> <기술개발촉각센서어레이:NC08> 를 위한 <센서:SI01> <네트워크:SI01> <기술개발유연:NC09> <촉각센서:NC08> 응용을 위한 <로봇:SH07> <플랫폼:SI05> <개발:SF01> <유연:NC09> <촉각센서:NC08> 성능평가를 위한 <그리퍼제작:SI02> 및 <테스트베드:SC15> <구축:SC07> <유연:NC09> 3D회로기술개발 <하이브리드:SE06> 첨가성형 <공정계획:EI02> 및 <제어기술:LC11> <개발:SF01> <탄성:NC05> <재료:SE05> 첨가 <가공:SI05> <기술개발하이브리드:SE06> 첨가성형장치 <개발:SF01> 신축성30 % 를갖는광경화성피복소재개발 <탄소나노튜브:NC10> [UNK] <그래핀:OB01> [UNK] <금속:ND03> 나노소재를이용한신축성30 % [UNK] 감도0. 1N을갖는센서소재개발 [SEP]'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_texts[7777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "647b4244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] <유연:NC09> <촉각센서신호처리:OB01> <기술개발촉각센서어레이:NC08> 를 위한 <센서:SI01> <네트워크:SI01> <기술개발유연:NC09> <촉각센서:NC08> 응용을 위한 <로봇:SH07> <플랫폼:SI05> <개발:SF01> <유연:NC09> <촉각센서:NC08> 성능평가를 위한 <그리퍼제작:SI02> 및 <테스트베드:SC15> <구축:SC07> <유연:NC09> 3D회로기술개발 <하이브리드:SE06> 첨가성형 <공정계획:EI02> 및 <제어기술:LC11> <개발:SF01> <탄성:NC05> <재료:SE05> 첨가 <가공:SI05> <기술개발하이브리드:SE06> 첨가성형장치 <개발:SF01> <신축성:NC08> 30 % 를갖는광경화성피복소재개발 <탄소나노튜브:NC10> [UNK] <그래핀:OB01> [UNK] <금속:ND03> 나노소재를이용한신축성30 % [UNK] <감도:LC04> 0. 1N을갖는센서소재개발 [SEP]'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label_texts[7777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9be385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['융합',\n",
       " '##공정의',\n",
       " '효율성',\n",
       " '평가',\n",
       " '및',\n",
       " '기본설계',\n",
       " '##안',\n",
       " '도출',\n",
       " '열변형',\n",
       " '##파',\n",
       " '##쇄',\n",
       " '선별',\n",
       " '기반',\n",
       " '고도',\n",
       " '##토양',\n",
       " '##세척',\n",
       " '##기술',\n",
       " '[UNK]',\n",
       " 'H',\n",
       " '##SF',\n",
       " '##기반',\n",
       " '고도',\n",
       " '##지',\n",
       " '##중',\n",
       " '##정화',\n",
       " '##기술에',\n",
       " '대한',\n",
       " '현장적용',\n",
       " '##성',\n",
       " '최종',\n",
       " '##평가',\n",
       " '기본설계',\n",
       " '##안',\n",
       " '및',\n",
       " '매뉴얼',\n",
       " '작성',\n",
       " '토양',\n",
       " '##환경',\n",
       " '##보전',\n",
       " '##법',\n",
       " '지',\n",
       " '##목',\n",
       " '##별',\n",
       " '우려',\n",
       " '##기준',\n",
       " '만족',\n",
       " 'Ex',\n",
       " '##s',\n",
       " '##itu',\n",
       " '열변형',\n",
       " '##파',\n",
       " '##쇄',\n",
       " '선별',\n",
       " '기반',\n",
       " '고도',\n",
       " '##토양',\n",
       " '##세척',\n",
       " '##공정',\n",
       " '효율',\n",
       " '##평가',\n",
       " 'Ins',\n",
       " '##itu',\n",
       " 'H',\n",
       " '##SF',\n",
       " '기반',\n",
       " '고도',\n",
       " '##지',\n",
       " '##중',\n",
       " '##정화',\n",
       " '##공정',\n",
       " '효율',\n",
       " '##평가',\n",
       " '3차년도',\n",
       " '현장실증',\n",
       " '##실험',\n",
       " '결과에',\n",
       " '##서',\n",
       " '도출한',\n",
       " '최적',\n",
       " '##인자를',\n",
       " '바탕으로',\n",
       " 'Pilot',\n",
       " '장치',\n",
       " '보완',\n",
       " '융합',\n",
       " '##공정',\n",
       " '운전',\n",
       " '##인자',\n",
       " '결정',\n",
       " '[UNK]',\n",
       " '성능평가',\n",
       " '후',\n",
       " '설계',\n",
       " '##보완',\n",
       " '정화',\n",
       " '##효율',\n",
       " '현장',\n",
       " '##검증']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(df_pred['input'].iloc[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9f378e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 563, 4, 463, 291, 4, 465, 4, 4, 507, 4, 4, 265, 217, 391, 4, 4, 4, 4, 4, 4, 4, 391, 4, 4, 4, 4, 4, 19, 20, 4, 4, 465, 4, 4, 297, 4, 471, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 507, 4, 4, 265, 217, 391, 4, 4, 4, 559, 4, 4, 4, 4, 4, 217, 391, 4, 4, 4, 4, 559, 4, 4, 233, 234, 4, 4, 4, 539, 4, 4, 4, 441, 4, 563, 4, 337, 4, 507, 4, 393, 4, 69, 4, 337, 4, 371, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred['target'].iloc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92f86ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['563',\n",
       " '4',\n",
       " '463',\n",
       " '291',\n",
       " '4',\n",
       " '465',\n",
       " '4',\n",
       " '4',\n",
       " '507',\n",
       " '4',\n",
       " '4',\n",
       " '265',\n",
       " '217',\n",
       " '391',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '391',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '19',\n",
       " '20',\n",
       " '4',\n",
       " '4',\n",
       " '465',\n",
       " '4',\n",
       " '4',\n",
       " '297',\n",
       " '4',\n",
       " '471',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '507',\n",
       " '4',\n",
       " '4',\n",
       " '265',\n",
       " '217',\n",
       " '391',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '559',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '217',\n",
       " '391',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '559',\n",
       " '4',\n",
       " '4',\n",
       " '233',\n",
       " '234',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '539',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '441',\n",
       " '4',\n",
       " '563',\n",
       " '4',\n",
       " '337',\n",
       " '4',\n",
       " '507',\n",
       " '4',\n",
       " '393',\n",
       " '4',\n",
       " '69',\n",
       " '4',\n",
       " '337',\n",
       " '4',\n",
       " '371',\n",
       " '4',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred['target'].iloc[200][2:-2].replace(',', '').split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
